# xc-proxy
批量爬取西刺代理的IP，并验证其有效性

   由于爬虫批量爬取都需要进行分页爬取，开始在爬取西刺代理的时候发现一个问题，每当爬取几页信息后，服务器就会返回一个503的错误（HTTP Error 503: Service Unavailable）。开始没有搞清楚到底是什么原因，代码写完进行批量爬取的时候发现，代码执行了几秒中后报错了。返回浏览器刷新网页的时候发现如图的情况，才反应过来，自己的IP被‘扳’了。经过仔细的分析，发现浏览器在每次发送请求的时候都带上了cookie值。通过对比每次请求的cookie值，猜测里面的一些值是使用js代码根据时间戳计算出来的。找到相应的js代码，发现自己并看不懂那段js代码，顿时感到绝望。
   ![ip被‘扳’之后的情况](https://github.com/anyuluo/xc-proxy/blob/master/4.jpg)
   就此作罢？
   可我并不甘心。脑海瞬间闪过的一丝想法，或许这就是灵感的到来吧！我不是在爬取代理IP么，为什么不用网站上的IP来做本次爬取的代理IP呢？哈哈！二话不说，咱们这就开干。
   首先，在西刺代理上面找两个可以使用的IP，并将其放入代理IP队列。
   第二步，从代理队列中随机选择一个IP值作为一次请求的代理IP，向服务器发送请求，得到服务器的响应数据。
   第三步，对服务器返回的数据进行解析，抽取出自己想要的IP信息。
   第四步，对抽取出来的IP进行有效性验证，如果IP可用，则将其加入代理队列（队列中的可用IP越来越多），并且保存到文件中。
   第五步，重复二至四步完成自己想要的内容的抽取。
   
   
   写完这个我又想到一个问题，每当解析出一个IP地址就对其进行有效性验证，中间会间隔一段相对长的时间。两次请求在这么长的时间间隔下，我们不使用代理是否也能成功的爬取到我们想要的内容呢。不过已经到深夜十一点了，我被‘扳’（被服务器拉黑）的IP一直没有恢复正常访问呢。
   
   欢迎大家给我提建议，欢迎大家共同来给项目添砖加瓦。
   
   申明：本项目仅是一种学习、技术、思路的分享，如有雷同，不胜荣幸。
   版权申明：本项目不具备任何商业目的，若有任何侵权，请联系我删除。
   
   （码字不易，转载、复制请注明作者）
